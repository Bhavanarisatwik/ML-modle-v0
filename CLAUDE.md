# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

ML-Modle v0 is the cybersecurity ML backend for DecoyVerse. It has two independently deployable services:
- **ML API** (`ml_api.py`) — FastAPI service on port 8000 that serves attack prediction models
- **Backend API** (`backend/main.py`) — FastAPI service on port 8001 that handles node management, event ingestion, alerts, decoys, and user auth — calls the ML API internally

A third component is the **endpoint deception agent** (`agent.py`) that deploys honeytokens on monitored machines and streams file-access events to the Backend API.

## Commands

### Install dependencies
```bash
pip install -r requirements.txt
```

### Generate training data + train models (run once)
```bash
python dataset_generator.py   # Creates training_data.csv (1000 synthetic records)
python train_model.py         # Trains & saves classifier.pkl, anomaly_model.pkl, scaler.pkl, etc.
```

### Run services
```bash
# ML API (port 8000)
python ml_api.py

# Backend API (port 8001) — from the backend/ directory
cd backend
uvicorn main:app --host 0.0.0.0 --port 8001 --reload
```

### Run agent
```bash
python agent.py           # Full mode (requires configured node)
python agent.py --demo    # Demo mode (no backend registration needed)
```

### Run tests
```bash
python test_cases.py          # 8 ML attack scenario tests (no server required)
python test_cases.py api      # Tests against live ML API on :8000
python test_api.py            # Backend API tests against :8001
python test_agent_attack.py   # End-to-end agent → backend → ML flow
python run_full_simulation.py # Full system simulation
```

## Environment Variables

**Backend API** (`backend/.env` or system env):
```
MONGODB_URI=mongodb+srv://user:pass@cluster.mongodb.net/decoyverse
JWT_SECRET_KEY=your_secret_here
ML_API_URL=https://ml-modle-v0-2.onrender.com   # or http://localhost:8000 for local
AUTH_ENABLED=True                                # Set False for demo/testing
FRONTEND_URL=https://your-vercel-app.vercel.app
```

**Notification integrations** (optional):
```
SLACK_WEBHOOK_URL=https://hooks.slack.com/...
SMTP_HOST=smtp.gmail.com
SMTP_USER=user@gmail.com
SMTP_PASSWORD=app_password
TWILIO_ACCOUNT_SID=...
TWILIO_AUTH_TOKEN=...
TWILIO_WHATSAPP_FROM=whatsapp:+14155238886
```

## Architecture

### Two-Service Design

```
Frontend (React :5173)
       │
       ├── Express backend (:5000)  ← Auth only (separate repo)
       │
       └── FastAPI Backend (:8001)  ← All data (this repo)
                │
                └── ML API (:8000)  ← Predictions (this repo)
```

The Backend API is what the DecoyVerse frontend's `apiClient` talks to. It calls the ML API internally via `backend/services/ml_service.py` — never directly from the frontend.

### ML Pipeline

**6 input features** (always in this order, defined in `feature_extractor.py`):
```
failed_logins, request_rate, commands_count, sql_payload, honeytoken_access, session_time
```

**Models** (`.pkl` files generated by `train_model.py`):
- `classifier.pkl` — RandomForest (100 trees, max_depth=15): predicts attack type
- `anomaly_model.pkl` — IsolationForest (contamination=0.1): anomaly score
- `scaler.pkl` — StandardScaler: normalizes features before inference
- `label_encoder.pkl` — encodes/decodes 5 attack classes
- Network variants: `network_classifier.pkl`, `network_scaler.pkl`, etc.

**Attack types**: `Normal`, `BruteForce`, `Injection`, `DataExfil`, `Recon`

**Risk score formula** (1–10, see `predict.py`):
```
anomaly_risk  = abs(anomaly_score) × 5
confidence_risk = confidence × (5 if not Normal else 2)
raw_risk = anomaly_risk × 0.4 + confidence_risk × 0.6
final_risk = raw_risk × multiplier[attack_type]  # Injection=1.3, DataExfil=1.2, BruteForce=1.1
```

### Event Ingestion Flow (Critical Path)

```
Agent file monitor detects honeytoken access
  → alert_sender.py converts event → 6 ML features
  → POST /api/agent-alert (with X-Node-Id, X-Node-Key headers)
  → backend/routes/agent.py validates node auth
  → ml_service.predict_attack() → POST /predict (15s timeout, fallback risk=9)
  → saves event + MLPrediction to MongoDB
  → if risk_score >= 7: creates alert, sends notifications
```

The feature conversion heuristics live in `backend/services/ml_service.py:_convert_to_ml_features()` — this is where file-access events become numeric ML inputs.

### Node Authentication
Agents authenticate via `X-Node-Id` and `X-Node-Key` headers. Validated by `backend/services/node_auth.py`. Set `AUTH_ENABLED=False` in config to bypass for testing.

### Data Scoping
Every MongoDB query in `backend/services/db_service.py` is filtered by `user_id`. Nodes belong to users; alerts, events, and decoys are always scoped to the owning user.

### Honeytoken Agent
`agent_setup.py` deploys 5 fake files into `system_cache/`:
- `aws_keys.txt`, `db_credentials.txt`, `api_keys.json`, `employee_data.csv`, `backup_file.zip`

`file_monitor.py` watches these files using `watchdog` (Windows `ReadDirectoryChangesW`). Events are deduplicated within a 5-second window. The manifest path defaults to `~/.cache/.honeytoken_manifest.json`.

## Key Files

| File | Purpose |
|------|---------|
| `ml_api.py` | ML prediction service (run standalone on :8000) |
| `predict.py` | `AttackPredictor` and `NetworkPredictor` classes |
| `feature_extractor.py` | Feature validation and normalization |
| `train_model.py` | Model training pipeline |
| `backend/main.py` | FastAPI app, lifespan, CORS, router mounting |
| `backend/config.py` | All configuration (MongoDB, JWT, ML URL, thresholds) |
| `backend/routes/agent.py` | `/api/agent-alert` — main event ingestion endpoint |
| `backend/services/ml_service.py` | ML API integration + feature conversion heuristics |
| `backend/services/db_service.py` | All MongoDB async operations |
| `backend/models/log_models.py` | All Pydantic models (15+) |
| `agent.py` | Endpoint deception agent orchestrator |
| `agent_config.py` | Agent config and backend registration |
| `file_monitor.py` | File access detection via watchdog |

## Deployment

Both services are deployed on **Render.com**:
- ML API: `https://ml-modle-v0-2.onrender.com` (serves `/predict`, `/health`)
- Backend API: `https://ml-modle-v0-1.onrender.com` (serves all `/api/*` routes)

Render free tier has cold starts (~30s). The `ml_service.py` uses a 15-second timeout and falls back to `risk_score=9` for honeytoken events and `risk_score=0` for other failures.

**Alert threshold**: `ALERT_RISK_THRESHOLD = 7` in `backend/config.py` — events with `risk_score >= 7` create a persistent alert and trigger notifications.
